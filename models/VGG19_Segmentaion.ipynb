{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##VGG19 model for FeelBeat with FER dataset"
      ],
      "metadata": {
        "id": "yxfwAlIHMQjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10GnHXyHLys9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e3e492-bb2b-4426-b88b-a30ab2c73e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import datetime\n",
        "import traceback\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "# sys.path.append(os.path.join(os.path.dirname(__file__), r'C:\\Users\\Tanzim Farhan\\Desktop\\FeelBeat\\VGG19_Segmentation\\VGG19_Segmentation'))\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch.multiprocessing as mp\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "Dz5P5d_aMQJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utils.py\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url"
      ],
      "metadata": {
        "id": "0X6iAGHpVE1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TdatqRdxe9Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet50.py\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"ResNet\",\n",
        "    \"resnet18\",\n",
        "    \"resnet34\",\n",
        "    \"resnet50\",\n",
        "    \"resnet101\",\n",
        "    \"resnet152\",\n",
        "    \"resnext50_32x4d\",\n",
        "    \"resnext101_32x8d\",\n",
        "    \"wide_resnet50_2\",\n",
        "    \"wide_resnet101_2\",\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    \"resnet18\": \"https://download.pytorch.org/models/resnet18-5c106cde.pth\",\n",
        "    \"resnet34\": \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\",\n",
        "    \"resnet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\",\n",
        "    \"resnet101\": \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\",\n",
        "    \"resnet152\": \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\",\n",
        "    \"resnext50_32x4d\": \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\",\n",
        "    \"resnext101_32x8d\": \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\",\n",
        "    \"wide_resnet50_2\": \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\",\n",
        "    \"wide_resnet101_2\": \"https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\",\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    exapansion = 1\n",
        "    __constants__ = [\"downsample\"]\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constatns__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(self,inplanes,planes,stride =1,downsample=None,groups=1,base_width =64,dilation =1,\n",
        "                 norm_layer = None):\n",
        "        super(Bottleneck,self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width,width,stride,groups,dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        def forward(x):\n",
        "            identity = x\n",
        "            out = self.conv1(x)\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv2(out)\n",
        "            out = self.bn2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "            out = self.conv3(out)\n",
        "            out = self.bn3(out)\n",
        "\n",
        "            if self.downsample is not None:\n",
        "                identity = self.downsample(x)\n",
        "\n",
        "            out += identity\n",
        "            out = self.relu(out)\n",
        "\n",
        "            return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self,block,layers,num_classes =1000, zero_init_residual = False,groups =1,width_per_group =64,replace_stride_with_dilation =None,norm_layer = None,inchannels =3):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # NOTE: strictly set the in_channels = 3 to load the pretrained model\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        # self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # NOTE: strictly set the num_classes = 1000 to load the pretrained model\n",
        "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self,block,planes,blocks,stride =1,dilate =False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "fQsg_tbAlSpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def up_pooling(in_channels,out_channels, kernel_size =2, stride =2):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(\n",
        "            in_channels,out_channels,kernel_size=kernel_size,stride=stride\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class Segmentation2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Segmentation2, self).__init__()\n",
        "        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))\n",
        "        \"\"\"\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n",
        "\n",
        "        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[3], 1),\n",
        "            nn.BatchNorm2d(filters[3]),\n",
        "        )\n",
        "\n",
        "        self.downsample4 = nn.Sequential(\n",
        "            conv1x1(filters[3], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.up_pool3 = up_pooling(filters[2], filters[1])\n",
        "        self.conv3 = block(filters[2], filters[1], downsample=self.downsample3)\n",
        "        self.conv4 = block(filters[1], filters[0], downsample=self.downsample4)\n",
        "\n",
        "         # init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "\n",
        "        x3 = self.up_pool3(x2)\n",
        "        x3 = torch.cat([x3, x1], dim=1)\n",
        "        x3 = self.conv3(x3)\n",
        "\n",
        "        x4 = self.conv4(x3)\n",
        "\n",
        "        output = torch.softmax(x4, dim=1)\n",
        "        # output = torch.sigmoid(x4)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Segmentation1(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,block =BasicBlock):\n",
        "        assert in_channels ==out_channels\n",
        "        super(Segmentation1,self).__init__()\n",
        "        filters = [in_channels,in_channels*2,in_channels*4,in_channels*8]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0],filters[1],1),\n",
        "            nn.BatchNorm2d(filters[1])\n",
        "        )\n",
        "\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1],filters[0],1),\n",
        "            nn.BatchNorm2d(filters[0])\n",
        "        )\n",
        "\n",
        "        self.conv2 = block(filters[1], filters[0], downsample=self.downsample2)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "            x1 = self.conv1(x)\n",
        "            x2 = self.conv2(x1)\n",
        "            output = torch.softmax(x2, dim=1)\n",
        "            # output = torch.sigmoid(x2)\n",
        "            return output\n",
        "\n",
        "class Segmentation3(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels, block = BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Segmentation3,self).__init__()\n",
        "        filters = [in_channels,in_channels*2,in_channels*4,in_channels*8]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0],filters[1],1),\n",
        "            nn.BatchNorm2d(filters[1])\n",
        "        )\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1],filters[2],1),\n",
        "            nn.BatchNorm2d(filters[2])\n",
        "        )\n",
        "\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            conv1x1(filters[2],filters[3],1),\n",
        "            nn.BatchNorm2d(filters[3])\n",
        "        )\n",
        "\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n",
        "        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)\n",
        "\n",
        "        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.downsample4 = nn.Sequential(\n",
        "            conv1x1(filters[3],filters[2],1),\n",
        "            nn.BatchNorm2d(filters[2])\n",
        "        )\n",
        "\n",
        "        self.downsample5 = nn.Sequential(\n",
        "            conv1x1(filters[2],filters[1],1),\n",
        "            nn.BatchNorm2d(filters[1])\n",
        "        )\n",
        "\n",
        "        self.downsample6 = nn.Sequential(\n",
        "            conv1x1(filters[1],filters[0],1),\n",
        "            nn.BatchNorm2d(filters[0])\n",
        "        )\n",
        "\n",
        "        self.up_pool4 = up_pooling(filters[3], filters[2])\n",
        "        self.conv4 = block(filters[3], filters[2], downsample=self.downsample4)\n",
        "        self.up_pool5 = up_pooling(filters[2], filters[1])\n",
        "        self.conv5 = block(filters[2], filters[1], downsample=self.downsample5)\n",
        "\n",
        "        self.conv6 = block(filters[1], filters[0], downsample=self.downsample6)\n",
        "\n",
        "        #init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "\n",
        "        x4 = self.up_pool4(x3)\n",
        "        x4 = torch.cat([x4, x2], dim=1)\n",
        "\n",
        "        x4 = self.conv4(x4)\n",
        "\n",
        "        x5 = self.up_pool5(x4)\n",
        "        x5 = torch.cat([x5, x1], dim=1)\n",
        "        x5 = self.conv5(x5)\n",
        "\n",
        "        x6 = self.conv6(x5)\n",
        "\n",
        "        output = torch.softmax(x6, dim=1)\n",
        "        # output = torch.sigmoid(x6)\n",
        "        return output\n",
        "\n",
        "class Segmentation4(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Segmentation4, self).__init__()\n",
        "        filters = [\n",
        "            in_channels,\n",
        "            in_channels * 2,\n",
        "            in_channels * 4,\n",
        "            in_channels * 8,\n",
        "            in_channels * 16,\n",
        "        ]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[3], 1),\n",
        "            nn.BatchNorm2d(filters[3]),\n",
        "        )\n",
        "\n",
        "        self.downsample4 = nn.Sequential(\n",
        "            conv1x1(filters[3], filters[4], 1),\n",
        "            nn.BatchNorm2d(filters[4]),\n",
        "        )\n",
        "\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n",
        "        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)\n",
        "        self.conv4 = block(filters[3], filters[4], downsample=self.downsample4)\n",
        "\n",
        "        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.downsample5 = nn.Sequential(\n",
        "            conv1x1(filters[4], filters[3], 1),\n",
        "            nn.BatchNorm2d(filters[3]),\n",
        "        )\n",
        "\n",
        "        self.downsample6 = nn.Sequential(\n",
        "            conv1x1(filters[3], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.downsample7 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample8 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[0], 1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.up_pool5 = up_pooling(filters[4], filters[3])\n",
        "        self.conv5 = block(filters[4], filters[3], downsample=self.downsample5)\n",
        "        self.up_pool6 = up_pooling(filters[3], filters[2])\n",
        "        self.conv6 = block(filters[3], filters[2], downsample=self.downsample6)\n",
        "        self.up_pool7 = up_pooling(filters[2], filters[1])\n",
        "        self.conv7 = block(filters[2], filters[1], downsample=self.downsample7)\n",
        "        self.conv8 = block(filters[1], filters[0], downsample=self.downsample8)\n",
        "\n",
        "        # init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "        p3 = self.down_pooling(x3)\n",
        "        x4 = self.conv4(p3)\n",
        "\n",
        "        x5 = self.up_pool5(x4)\n",
        "        x5 = torch.cat([x5, x3], dim=1)\n",
        "        x5 = self.conv5(x5)\n",
        "\n",
        "        x6 = self.up_pool6(x5)\n",
        "        x6 = torch.cat([x6, x2], dim=1)\n",
        "        x6 = self.conv6(x6)\n",
        "\n",
        "        x7 = self.up_pool7(x6)\n",
        "        x7 = torch.cat([x7, x1], dim=1)\n",
        "        x7 = self.conv7(x7)\n",
        "\n",
        "        x8 = self.conv8(x7)\n",
        "\n",
        "        output = torch.softmax(x8, dim=1)\n",
        "        # output = torch.sigmoid(x8)\n",
        "        return output\n",
        "\n",
        "\n",
        "def segmentation(in_channels, out_channels, depth, block=BasicBlock):\n",
        "    if depth == 1:\n",
        "        return Segmentation1(in_channels, out_channels, block)\n",
        "    elif depth == 2:\n",
        "        return Segmentation2(in_channels, out_channels, block)\n",
        "    elif depth == 3:\n",
        "        return Segmentation3(in_channels, out_channels, block)\n",
        "    elif depth == 4:\n",
        "        return Segmentation4(in_channels, out_channels, block)\n",
        "    else:\n",
        "        traceback.print_exc()\n",
        "        raise Exception(\"depth need to be from 1-4\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "84tVNZVw223M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = [\n",
        "    \"VGG\",\n",
        "    \"vgg19_bn\",\n",
        "]\n",
        "\n",
        "model_urls = {\n",
        "    \"vgg19_bn\": \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\",\n",
        "}\n",
        "\n",
        "\n",
        "new_dict = ['conv1.weight','conv1.bias',\n",
        "            'bn1.weight','bn1.bias','bn1.running_mean','bn1.running_var',\n",
        "            'conv2.weight','conv2.bias',\n",
        "            'bn2.weight','bn2.bias','bn2.running_mean','bn2.running_var',\n",
        "            'conv3.weight','conv3.bias',\n",
        "            'bn3.weight','bn3.bias','bn3.running_mean','bn3.running_var',\n",
        "            'conv4.weight','conv4.bias',\n",
        "            'bn4.weight','bn4.bias','bn4.running_mean','bn4.running_var',\n",
        "            'conv5.weight','conv5.bias',\n",
        "            'bn5.weight','bn5.bias','bn5.running_mean','bn5.running_var',\n",
        "            'conv6.weight','conv6.bias',\n",
        "            'bn6.weight','bn6.bias','bn6.running_mean','bn6.running_var',\n",
        "            'conv7.weight','conv7.bias',\n",
        "            'bn7.weight','bn7.bias','bn7.running_mean','bn7.running_var',\n",
        "            'conv8.weight','conv8.bias',\n",
        "            'bn8.weight','bn8.bias','bn8.running_mean','bn8.running_var',\n",
        "            'conv9.weight','conv9.bias',\n",
        "            'bn9.weight','bn9.bias','bn9.running_mean','bn9.running_var',\n",
        "            'conv10.weight','conv10.bias',\n",
        "            'bn10.weight','bn10.bias','bn10.running_mean','bn10.running_var',\n",
        "            'conv11.weight','conv11.bias',\n",
        "            'bn11.weight','bn11.bias','bn11.running_mean','bn11.running_var',\n",
        "            'conv12.weight','conv12.bias',\n",
        "            'bn12.weight','bn12.bias','bn12.running_mean','bn12.running_var',\n",
        "            'conv13.weight','conv13.bias',\n",
        "            'bn13.weight','bn13.bias','bn13.running_mean','bn13.running_var',\n",
        "            'conv14.weight','conv14.bias',\n",
        "            'bn14.weight','bn14.bias','bn14.running_mean','bn14.running_var',\n",
        "            'conv15.weight','conv15.bias',\n",
        "            'bn15.weight','bn15.bias','bn15.running_mean','bn15.running_var',\n",
        "            'conv16.weight','conv16.bias',\n",
        "            'bn16.weight','bn16.bias','bn16.running_mean','bn16.running_var'\n",
        "            ]\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=7, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.maxp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "        self.maxp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.relu5 = nn.ReLU(inplace=True)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.relu7= nn.ReLU(inplace=True)\n",
        "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(256)\n",
        "        self.relu8 = nn.ReLU(inplace=True)\n",
        "        self.maxp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv9 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(512)\n",
        "        self.relu9 = nn.ReLU(inplace=True)\n",
        "        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn10 = nn.BatchNorm2d(512)\n",
        "        self.relu10 = nn.ReLU(inplace=True)\n",
        "        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(512)\n",
        "        self.relu11 = nn.ReLU(inplace=True)\n",
        "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(512)\n",
        "        self.relu12 = nn.ReLU(inplace=True)\n",
        "        self.maxp4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn13 = nn.BatchNorm2d(512)\n",
        "        self.relu13 = nn.ReLU(inplace=True)\n",
        "        self.conv14 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn14 = nn.BatchNorm2d(512)\n",
        "        self.relu14 = nn.ReLU(inplace=True)\n",
        "        self.conv15 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn15 = nn.BatchNorm2d(512)\n",
        "        self.relu15 = nn.ReLU(inplace=True)\n",
        "        self.conv16 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn16 = nn.BatchNorm2d(512)\n",
        "        self.relu16 = nn.ReLU(inplace=True)\n",
        "        self.maxp5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxp1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.maxp2(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.bn6(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.bn7(x)\n",
        "        x = self.relu7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.bn8(x)\n",
        "        x = self.relu8(x)\n",
        "        x = self.maxp3(x)\n",
        "        x = self.conv9(x)\n",
        "        x = self.bn9(x)\n",
        "        x = self.relu9(x)\n",
        "        x = self.conv10(x)\n",
        "        x = self.bn10(x)\n",
        "        x = self.relu10(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.bn11(x)\n",
        "        x = self.relu11(x)\n",
        "        x = self.conv12(x)\n",
        "        x = self.bn12(x)\n",
        "        x = self.relu12(x)\n",
        "        x = self.maxp4(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.bn13(x)\n",
        "        x = self.relu13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.bn14(x)\n",
        "        x = self.relu14(x)\n",
        "        x = self.conv15(x)\n",
        "        x = self.bn15(x)\n",
        "        x = self.relu15(x)\n",
        "        x = self.conv16(x)\n",
        "        x = self.bn16(x)\n",
        "        x = self.relu16(x)\n",
        "        x = self.maxp5(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def _vgg(arch, pretrained, progress, **kwargs):\n",
        "\n",
        "    if pretrained:\n",
        "        kwargs[\"init_weights\"] = False\n",
        "\n",
        "    model = VGG(in_channels=3, num_classes=1000, init_weights=True)\n",
        "    if pretrained:\n",
        "\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        tuple_list = list(state_dict.items())\n",
        "\n",
        "        for i in range(len(new_dict)):\n",
        "            state_dict = OrderedDict([(new_dict[i], v) if k == tuple_list[i][0] else (k, v) for k, v in state_dict.items()])\n",
        "\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg19_bn(pretrained=True, progress=True, **kwargs):\n",
        "\n",
        "    model = _vgg(\"vgg19_bn\", pretrained, progress, **kwargs)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(512 * 7 * 7, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 7),\n",
        "    )\n",
        "    print(model)\n",
        "    return model"
      ],
      "metadata": {
        "id": "G2hAxvnu3FWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imgaug import augmenters as iaa\n",
        "\n",
        "seg = iaa.Sequential(\n",
        "    [\n",
        "        # iaa.Fliplr(p=0.5, deterministic=True),\n",
        "        iaa.Fliplr(p=0.5),\n",
        "        # iaa.Affine(rotate=(-30, 30), deterministic=True),\n",
        "        iaa.Affine(rotate=(-30, 30)),\n",
        "        # iaa.GaussianBlur(sigma=(0., 4.0), deterministic=True),\n",
        "        # iaa.Dropout((0., 0.15), deterministic=True),\n",
        "        # iaa.Add((-25, 25), deterministic=True),\n",
        "        # iaa.CropAndPad(percent=(-0.05, 0.1), pad_cval=(0, 255), deterministic=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "7lPBUwLl3N8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "EMOTION_DICT = {\n",
        "    0: \"angry\",\n",
        "    1: \"disgust\",\n",
        "    2: \"fear\",\n",
        "    3: \"happy\",\n",
        "    4: \"sad\",\n",
        "    5: \"surprise\",\n",
        "    6: \"neutral\",\n",
        "}\n",
        "\n",
        "class FER2013(Dataset):\n",
        "    def __init__(self,stage, configs, tta =False, tta_size =48):\n",
        "        self._stage = stage\n",
        "        self._configs = configs\n",
        "        self._tta = tta\n",
        "        self._tta_size = tta_size\n",
        "\n",
        "        self._image_size = (configs[\"image_size\"], configs[\"image_size\"])\n",
        "\n",
        "        self._data = pd.read_csv(os.path.join(configs[\"data_path\"], \"{}.csv\".format(stage)))\n",
        "\n",
        "        self._pixels = self._data[\"pixels\"].tolist()\n",
        "        self._emotions = pd.get_dummies(self._data[\"emotion\"])\n",
        "\n",
        "        self._transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def is_tta(self):\n",
        "        return self._tta == True\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._pixels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self._pixels[idx]\n",
        "        pixels = list(map(int, pixels.split(\" \")))\n",
        "        image = np.asarray(pixels).reshape(48, 48)\n",
        "        image = image.astype(np.uint8)\n",
        "\n",
        "        image = cv2.resize(image, self._image_size)\n",
        "        image = np.dstack([image] * 3)\n",
        "\n",
        "        if self._stage == \"train\":\n",
        "            image = seg(image=image)\n",
        "\n",
        "        image = self._transform(image)\n",
        "        target = self._emotions.iloc[idx].idxmax()\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def fer2013(stage, configs=None, tta=False, tta_size=48):\n",
        "    return FER2013(stage, configs, tta, tta_size)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = FER2013(\n",
        "        \"train\",{\n",
        "            \"data_path\" :\"/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/fer_data\",\n",
        "            \"image_size\" : 224,\n",
        "            \"in_channels\" : 3,\n",
        "        },\n",
        "            )\n",
        "\n",
        "    import cv2\n",
        "    targets = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "          image, target = data[i]\n",
        "\n",
        "          # Assume image is a PyTorch tensor; adjust as necessary for your data structure\n",
        "          # Convert to numpy array\n",
        "          if isinstance(image, torch.Tensor):\n",
        "              image = image.numpy()\n",
        "\n",
        "          # If the tensor was on GPU, first move it to CPU\n",
        "          # image = image.cpu().numpy() # Uncomment if your tensors are on GPU\n",
        "\n",
        "          # If the image has 3 channels and is in (C, H, W) format, transpose to (H, W, C)\n",
        "          if image.ndim == 3 and image.shape[0] == 3:\n",
        "              image = image.transpose(1, 2, 0)\n",
        "\n",
        "          # Ensure the image is in uint8\n",
        "          if image.dtype != np.uint8:\n",
        "              image = (image * 255).astype(np.uint8)\n",
        "\n",
        "          # Write the image\n",
        "          cv2.imwrite(f\"debug/{i}.png\", image)\n",
        "\n",
        "          if i == 200:\n",
        "              break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wPaZsOfb3XFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        correct = pred.eq(target).float().sum(0)\n",
        "        acc = correct * 100 / batch_size\n",
        "    return acc, pred, target  # Return acc directly as a tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "0GXfMvVC3br5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "EPS = 1e-10\n",
        "\n",
        "\n",
        "def nanmean(x):\n",
        "    return torch.mean(x[x == x])\n",
        "\n",
        "\n",
        "def _fast_hist(true, pred, num_classes):\n",
        "    mask = (true >= 0) & (true < num_classes)\n",
        "    hist = (\n",
        "        torch.bincount(\n",
        "            num_classes * true[mask] + pred[mask],\n",
        "            minlength=num_classes ** 2,\n",
        "        )\n",
        "        .reshape(num_classes, num_classes)\n",
        "        .float()\n",
        "    )\n",
        "    return hist\n",
        "\n",
        "\n",
        "def overall_pixel_accuracy(hist):\n",
        "\n",
        "    correct = torch.diag(hist).sum()\n",
        "    total = hist.sum()\n",
        "    overall_acc = correct / (total + EPS)\n",
        "    return overall_acc\n",
        "\n",
        "\n",
        "def per_class_pixel_accuracy(hist):\n",
        "\n",
        "    correct_per_class = torch.diag(hist)\n",
        "    total_per_class = hist.sum(dim=1)\n",
        "    per_class_acc = correct_per_class / (total_per_class + EPS)\n",
        "    avg_per_class_acc = nanmean(per_class_acc)\n",
        "    return avg_per_class_acc\n",
        "\n",
        "\n",
        "def jaccard_index(hist):\n",
        "\n",
        "    A_inter_B = torch.diag(hist)\n",
        "    A = hist.sum(dim=1)\n",
        "    B = hist.sum(dim=0)\n",
        "    jaccard = A_inter_B / (A + B - A_inter_B + EPS)\n",
        "    avg_jacc = nanmean(jaccard)\n",
        "    return avg_jacc\n",
        "\n",
        "\n",
        "def dice_coefficient(hist):\n",
        "\n",
        "    A_inter_B = torch.diag(hist)\n",
        "    A = hist.sum(dim=1)\n",
        "    B = hist.sum(dim=0)\n",
        "    dice = (2 * A_inter_B) / (A + B + EPS)\n",
        "    avg_dice = nanmean(dice)\n",
        "    return avg_dice\n",
        "\n",
        "\n",
        "def eval_metrics(true, pred, num_classes):\n",
        "\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    hist = torch.zeros((num_classes, num_classes)).cuda()\n",
        "\n",
        "    for t, p in zip(true, pred):\n",
        "        hist += _fast_hist(t.flatten(), p.flatten(), num_classes)\n",
        "    overall_acc = overall_pixel_accuracy(hist)\n",
        "    avg_per_class_acc = per_class_pixel_accuracy(hist)\n",
        "    avg_jacc = jaccard_index(hist)\n",
        "    avg_dice = dice_coefficient(hist)\n",
        "    return overall_acc, avg_per_class_acc, avg_jacc, avg_dice"
      ],
      "metadata": {
        "id": "ENh78nCs3gen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(images):\n",
        "    if not isinstance(images, list):\n",
        "        images = [images]\n",
        "    return torch.stack(images, 0)"
      ],
      "metadata": {
        "id": "Ei3G_AIi3k-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAdam(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        self.buffer = [[None, None, None] for ind in range(10)]\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                buffered = self.buffer[int(state[\"step\"] % 10)]\n",
        "                if state[\"step\"] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state[\"step\"]\n",
        "                    beta2_t = beta2 ** state[\"step\"]\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = (\n",
        "                            group[\"lr\"]\n",
        "                            * math.sqrt(\n",
        "                                (1 - beta2_t)\n",
        "                                * (N_sma - 4)\n",
        "                                / (N_sma_max - 4)\n",
        "                                * (N_sma - 2)\n",
        "                                / N_sma\n",
        "                                * N_sma_max\n",
        "                                / (N_sma_max - 2)\n",
        "                            )\n",
        "                            / (1 - beta1 ** state[\"step\"])\n",
        "                        )\n",
        "                    else:\n",
        "                        step_size = group[\"lr\"] / (1 - beta1 ** state[\"step\"])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "\n",
        "                if N_sma >= 5:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class PlainRAdam(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "        super(PlainRAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(PlainRAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                beta2_t = beta2 ** state[\"step\"]\n",
        "                N_sma_max = 2 / (1 - beta2) - 1\n",
        "                N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    step_size = (\n",
        "                        group[\"lr\"]\n",
        "                        * math.sqrt(\n",
        "                            (1 - beta2_t)\n",
        "                            * (N_sma - 4)\n",
        "                            / (N_sma_max - 4)\n",
        "                            * (N_sma - 2)\n",
        "                            / N_sma\n",
        "                            * N_sma_max\n",
        "                            / (N_sma_max - 2)\n",
        "                        )\n",
        "                        / (1 - beta1 ** state[\"step\"])\n",
        "                    )\n",
        "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                else:\n",
        "                    step_size = group[\"lr\"] / (1 - beta1 ** state[\"step\"])\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "    def __init__(\n",
        "        self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup=0\n",
        "    ):\n",
        "        defaults = dict(\n",
        "            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, warmup=warmup\n",
        "        )\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        \"Adam does not support sparse gradients, please consider SparseAdam instead\"\n",
        "                    )\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
        "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
        "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
        "\n",
        "                if group[\"warmup\"] > state[\"step\"]:\n",
        "                    scheduled_lr = 1e-8 + state[\"step\"] * group[\"lr\"] / group[\"warmup\"]\n",
        "                else:\n",
        "                    scheduled_lr = group[\"lr\"]\n",
        "\n",
        "                step_size = group[\"lr\"] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p_data_fp32.add_(-group[\"weight_decay\"] * scheduled_lr, p_data_fp32)\n",
        "\n",
        "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "VxeijTk73nxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMO_DICT = {0: \"ne\", 1: \"an\", 2: \"di\", 3: \"fe\", 4: \"ha\", 5: \"sa\", 6: \"su\"}\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        print(\"Trainer initialized\")\n",
        "\n",
        "class FER2013Trainer:\n",
        "    def __init__(self,model,train_set,val_set,test_set, configs):\n",
        "        super(FER2013Trainer).__init__()\n",
        "        print(\"Start trainer\")\n",
        "        print(configs)\n",
        "\n",
        "        #load config\n",
        "        self._configs = configs\n",
        "        self._lr = self._configs[\"lr\"]\n",
        "        self._batch_size = self._configs[\"batch_size\"]\n",
        "        self._momentum = self._configs[\"momentum\"]\n",
        "        self._weight_decay = self._configs[\"weight_decay\"]\n",
        "        self._distributed = self._configs[\"distributed\"]\n",
        "        self._num_workers = self._configs[\"num_workers\"]\n",
        "        self._device = torch.device(self._configs[\"device\"])\n",
        "        self._max_epoch_num = self._configs[\"max_epoch_num\"]\n",
        "        self._max_plateau_count = self._configs[\"max_plateau_count\"]\n",
        "\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "        #load dataloader nad model\n",
        "        self._train_set = train_set\n",
        "        self._val_set = val_set\n",
        "        self._test_set = test_set\n",
        "        self._model = model.to(self._device)\n",
        "        print(self._device)\n",
        "        if self._distributed ==1:\n",
        "            self._model = nn.DataParallel(self._model).to(self._device)\n",
        "            self._train_loader = DataLoader(\n",
        "                self._train_set,\n",
        "                batch_size=self._batch_size,\n",
        "                num_workers=self._num_workers,\n",
        "                pin_memory=True,\n",
        "                shuffle=True,\n",
        "                worker_init_fn=lambda x: np.random.seed(x),\n",
        "            )\n",
        "            self._val_loader = DataLoader(\n",
        "                self._val_set,\n",
        "                batch_size=self._batch_size,\n",
        "                num_workers=self._num_workers,\n",
        "                pin_memory=True,\n",
        "                shuffle=False,\n",
        "                worker_init_fn=lambda x: np.random.seed(x),\n",
        "            )\n",
        "\n",
        "            self._test_loader = DataLoader(\n",
        "                self._test_set,\n",
        "                batch_size=1,\n",
        "                num_workers=self._num_workers,\n",
        "                pin_memory=True,\n",
        "                shuffle=False,\n",
        "                worker_init_fn=lambda x: np.random.seed(x),\n",
        "            )\n",
        "        else:\n",
        "            self._train_loader = DataLoader(\n",
        "                self._train_set,\n",
        "                batch_size=self._batch_size,\n",
        "                num_workers=self._num_workers,\n",
        "                pin_memory=True,\n",
        "                shuffle=True,\n",
        "            )\n",
        "            self._val_loader = DataLoader(\n",
        "                self._val_set,\n",
        "                batch_size=self._batch_size,\n",
        "                num_workers=self._num_workers,\n",
        "                pin_memory=True,\n",
        "                shuffle=False,\n",
        "            )\n",
        "            self._test_loader = DataLoader(\n",
        "                self._test_set,\n",
        "                batch_size=1,\n",
        "                num_workers=self._num_workers,\n",
        "                pin_memory=True,\n",
        "                shuffle=False,\n",
        "            )\n",
        "        # define loss function (criterion) and optimizer\n",
        "        class_weights = [\n",
        "            1.02660468,\n",
        "            9.40661861,\n",
        "            1.00104606,\n",
        "            0.56843877,\n",
        "            0.84912748,\n",
        "            1.29337298,\n",
        "            0.82603942,\n",
        "        ]\n",
        "        class_weights = torch.FloatTensor(np.array(class_weights))\n",
        "\n",
        "        if self._configs[\"weighted_loss\"] == 0:\n",
        "            self._criterion = nn.CrossEntropyLoss().to(self._device)\n",
        "        else:\n",
        "            self._criterion = nn.CrossEntropyLoss(class_weights).to(self._device)\n",
        "\n",
        "        self._optimizer = RAdam(\n",
        "            params=self._model.parameters(),\n",
        "            lr=self._lr,\n",
        "            weight_decay=self._weight_decay,\n",
        "        )\n",
        "\n",
        "        self._scheduler = ReduceLROnPlateau(\n",
        "            self._optimizer,\n",
        "            patience=self._configs[\"plateau_patience\"],\n",
        "            min_lr=1e-6,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "        # training info\n",
        "        self._start_time = datetime.datetime.now()\n",
        "        self._start_time = self._start_time.replace(microsecond=0)\n",
        "\n",
        "        log_dir = os.path.join(\n",
        "            self._configs[\"cwd\"],\n",
        "            self._configs[\"log_dir\"],\n",
        "            \"{}_{}_{}\".format(\n",
        "                self._configs[\"arch\"],\n",
        "                self._configs[\"model_name\"],\n",
        "                self._start_time.strftime(\"%Y%b%d_%H.%M\"),\n",
        "            ),\n",
        "        )\n",
        "        self._writer = SummaryWriter(log_dir)\n",
        "        self._train_loss_list = []\n",
        "        self._train_acc_list = []\n",
        "        self._val_loss_list = []\n",
        "        self._val_acc_list = []\n",
        "        self._best_val_loss = 1e9\n",
        "        self._best_val_acc = 0\n",
        "        self._best_train_loss = 1e9\n",
        "        self._best_train_acc = 0\n",
        "        self._test_acc = 0.0\n",
        "        self._plateau_count = 0\n",
        "        self._current_epoch_num = 0\n",
        "\n",
        "        self._checkpoint_dir = os.path.join(self._configs[\"cwd\"], self._configs[\"checkpoint_dir\"])\n",
        "        if not os.path.exists(self._checkpoint_dir):\n",
        "            os.makedirs(self._checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "        self._checkpoint_path = os.path.join(\n",
        "            self._checkpoint_dir,\n",
        "            \"{}_{}_{}\".format(\n",
        "                self._configs[\"arch\"],\n",
        "                self._configs[\"model_name\"],\n",
        "                self._start_time.strftime(\"%Y%b%d_%H.%M\"),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def _train(self):\n",
        "        self._model.train()\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        for i, (images, targets) in tqdm(\n",
        "            enumerate(self._train_loader), total=len(self._train_loader), leave=False):\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            targets = targets.cuda(non_blocking=True)\n",
        "\n",
        "            # compute output, measure accuracy and record loss\n",
        "            outputs = self._model(images)\n",
        "\n",
        "            loss = self._criterion(outputs, targets)\n",
        "            acc = accuracy(outputs, targets)[0]\n",
        "            # acc = eval_metrics(targets, outputs, 2)[0]\n",
        "\n",
        "            acc, pred, target = accuracy(outputs, targets)\n",
        "            train_acc += acc.item()  # Now acc is directly a tensor\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_acc += acc.item()\n",
        "\n",
        "            # compute gradient and do SGD step\n",
        "            self._optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self._optimizer.step()\n",
        "\n",
        "        i += 1\n",
        "        self._train_loss_list.append(train_loss / i)\n",
        "        self._train_acc_list.append(train_acc / i)\n",
        "\n",
        "    def _val(self):\n",
        "        self._model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (images, targets) in tqdm(\n",
        "                enumerate(self._val_loader), total=len(self._val_loader), leave=False\n",
        "            ):\n",
        "                images = images.cuda(non_blocking=True)\n",
        "                targets = targets.cuda(non_blocking=True)\n",
        "\n",
        "                # compute output, measure accuracy and record loss\n",
        "                outputs = self._model(images)\n",
        "\n",
        "                loss = self._criterion(outputs, targets)\n",
        "                acc = accuracy(outputs, targets)[0]\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_acc += acc.item()\n",
        "\n",
        "            i += 1\n",
        "            self._val_loss_list.append(val_loss / i)\n",
        "            self._val_acc_list.append(val_acc / i)\n",
        "\n",
        "    def _calc_acc_on_private_test(self):\n",
        "        self._model.eval()\n",
        "        test_acc = 0.0\n",
        "        print(\"Calc acc on private test..\")\n",
        "        f = open(\"private_test_log.txt\", \"w\")\n",
        "        with torch.no_grad():\n",
        "            for i, (images, targets) in tqdm(\n",
        "                enumerate(self._test_loader), total=len(self._test_loader), leave=False\n",
        "            ):\n",
        "\n",
        "                images = images.cuda(non_blocking=True)\n",
        "                targets = targets.cuda(non_blocking=True)\n",
        "\n",
        "                outputs = self._model(images)\n",
        "                print(outputs.shape, outputs)\n",
        "                acc = accuracy(outputs, targets)[0]\n",
        "                test_acc += acc.item()\n",
        "                f.writelines(\"{}_{}\\n\".format(i, acc.item()))\n",
        "\n",
        "            test_acc = test_acc / (i + 1)\n",
        "        print(\"Accuracy on private test: {:.3f}\".format(test_acc))\n",
        "        f.close()\n",
        "        return test_acc\n",
        "\n",
        "    def _calc_acc_on_private_test_with_tta(self):\n",
        "\n",
        "        pred_list = np.array([])\n",
        "        target_list = np.array([])\n",
        "\n",
        "        self._model.eval()\n",
        "        test_acc = 0.0\n",
        "        print(\"Calc acc on private test with tta..\")\n",
        "        f = open(\n",
        "            \"private_test_log_{}_{}.txt\".format(\n",
        "                self._configs[\"arch\"], self._configs[\"model_name\"]\n",
        "            ),\n",
        "            \"w\",\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx in tqdm(\n",
        "                range(len(self._test_set)), total=len(self._test_set), leave=False\n",
        "            ):\n",
        "                images, targets = self._test_set[idx]\n",
        "                targets = torch.LongTensor([targets])\n",
        "\n",
        "                images = make_batch(images)\n",
        "                images = images.cuda(non_blocking=True)\n",
        "                targets = targets.cuda(non_blocking=True)\n",
        "\n",
        "                outputs = self._model(images)\n",
        "                outputs = F.softmax(outputs, 1)\n",
        "\n",
        "\n",
        "                outputs = torch.sum(outputs, 0)\n",
        "\n",
        "                outputs = torch.unsqueeze(outputs, 0)\n",
        "\n",
        "                acc, pred, target = accuracy(outputs, targets)\n",
        "                pred_list = np.append(pred_list, int(pred.cpu().numpy()[0]))\n",
        "                target_list = np.append(target_list, int(target.cpu().numpy()[0]))\n",
        "\n",
        "                np.save('/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/results', pred_list)\n",
        "                np.save('/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/results', target_list)\n",
        "\n",
        "\n",
        "\n",
        "        #         test_acc += acc.item()\n",
        "        #         f.writelines(\"{}_{}\\n\".format(idx, acc.item()))\n",
        "\n",
        "        #     test_acc = test_acc / (idx + 1)\n",
        "        # print(\"Accuracy on private test with tta: {:.3f}\".format(test_acc))\n",
        "\n",
        "        f.close()\n",
        "        return test_acc\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"make a training job\"\"\"\n",
        "\n",
        "        try:\n",
        "            while not self._is_stop():\n",
        "                self._increase_epoch_num()\n",
        "                self._train()\n",
        "                self._val()\n",
        "\n",
        "                self._update_training_state()\n",
        "                self._logging()\n",
        "        except KeyboardInterrupt:\n",
        "            traceback.print_exc()\n",
        "            pass\n",
        "\n",
        "        # training stop\n",
        "        try:\n",
        "            if not self._test_set.is_tta():\n",
        "                self._test_acc = self._calc_acc_on_private_test()\n",
        "            else:\n",
        "                self._test_acc = self._calc_acc_on_private_test_with_tta()\n",
        "\n",
        "            # self._test_acc = self._calc_acc_on_private_test()\n",
        "            self._save_weights()\n",
        "        except Exception as e:\n",
        "            traceback.print_exc()\n",
        "            pass\n",
        "\n",
        "        consume_time = str(datetime.datetime.now() - self._start_time)\n",
        "        self._writer.add_text(\n",
        "            \"Summary\",\n",
        "            \"Converged after {} epochs, consume {}\".format(\n",
        "                self._current_epoch_num, consume_time[:-7]\n",
        "            ),\n",
        "        )\n",
        "        self._writer.add_text(\n",
        "            \"Results\", \"Best validation accuracy: {:.3f}\".format(self._best_val_acc)\n",
        "        )\n",
        "        self._writer.add_text(\n",
        "            \"Results\", \"Best training accuracy: {:.3f}\".format(self._best_train_acc)\n",
        "        )\n",
        "        self._writer.add_text(\n",
        "            \"Results\", \"Private test accuracy: {:.3f}\".format(self._test_acc)\n",
        "        )\n",
        "        self._writer.close()\n",
        "\n",
        "    def _update_training_state(self):\n",
        "        if self._val_acc_list[-1] > self._best_val_acc:\n",
        "            self._save_weights()\n",
        "            self._plateau_count = 0\n",
        "            self._best_val_acc = self._val_acc_list[-1]\n",
        "            self._best_val_loss = self._val_loss_list[-1]\n",
        "            self._best_train_acc = self._train_acc_list[-1]\n",
        "            self._best_train_loss = self._train_loss_list[-1]\n",
        "        else:\n",
        "            self._plateau_count += 1\n",
        "\n",
        "        # self._scheduler.step(self._train_loss_list[-1])\n",
        "        self._scheduler.step(100 - self._val_acc_list[-1])\n",
        "        # self._scheduler.step()\n",
        "\n",
        "    def _logging(self):\n",
        "        consume_time = str(datetime.datetime.now() - self._start_time)\n",
        "\n",
        "        message = \"\\nE{:03d}  {:.3f}/{:.3f}/{:.3f} train_acc{:.3f}/val_acc{:.3f}/Best val: {:.3f} | p{:02d}  Time {}\\n\".format(\n",
        "            self._current_epoch_num,\n",
        "            self._train_loss_list[-1],\n",
        "            self._val_loss_list[-1],\n",
        "            self._best_val_loss,\n",
        "            self._train_acc_list[-1],\n",
        "            self._val_acc_list[-1],\n",
        "            self._best_val_acc,\n",
        "            self._plateau_count,\n",
        "            consume_time[:-7],\n",
        "        )\n",
        "\n",
        "        self._writer.add_scalar(\n",
        "            \"Accuracy/Train\", self._train_acc_list[-1], self._current_epoch_num\n",
        "        )\n",
        "        self._writer.add_scalar(\n",
        "            \"Accuracy/Val\", self._val_acc_list[-1], self._current_epoch_num\n",
        "        )\n",
        "        self._writer.add_scalar(\n",
        "            \"Loss/Train\", self._train_loss_list[-1], self._current_epoch_num\n",
        "        )\n",
        "        self._writer.add_scalar(\n",
        "            \"Loss/Val\", self._val_loss_list[-1], self._current_epoch_num\n",
        "        )\n",
        "\n",
        "        print(message)\n",
        "\n",
        "    def _is_stop(self):\n",
        "        \"\"\"check stop condition\"\"\"\n",
        "        return (\n",
        "            self._plateau_count > self._max_plateau_count\n",
        "            or self._current_epoch_num > self._max_epoch_num\n",
        "        )\n",
        "\n",
        "    def _increase_epoch_num(self):\n",
        "        self._current_epoch_num += 1\n",
        "\n",
        "    def _save_weights(self, test_acc=0.0):\n",
        "        if self._distributed == 0:\n",
        "            state_dict = self._model.state_dict()\n",
        "        else:\n",
        "            state_dict = self._model.module.state_dict()\n",
        "\n",
        "        state = {\n",
        "            **self._configs,\n",
        "            \"net\": state_dict,\n",
        "            \"best_val_loss\": self._best_val_loss,\n",
        "            \"best_val_acc\": self._best_val_acc,\n",
        "            \"best_train_loss\": self._best_train_loss,\n",
        "            \"best_train_acc\": self._best_train_acc,\n",
        "            \"train_losses\": self._train_loss_list,\n",
        "            \"val_loss_list\": self._val_loss_list,\n",
        "            \"train_acc_list\": self._train_acc_list,\n",
        "            \"val_acc_list\": self._val_acc_list,\n",
        "            \"test_acc\": self._test_acc,\n",
        "        }\n",
        "\n",
        "        torch.save(state, self._checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6GqE1HzN3sGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imgaug\n",
        "import random\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "imgaug.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def main(config_path):\n",
        "    configs = json.load(open(config_path))\n",
        "    configs[\"cwd\"] = os.getcwd()\n",
        "    model = get_model(configs)\n",
        "    train_set, val_set, test_set = get_dataset(configs)\n",
        "    trainer = FER2013Trainer(model, train_set, val_set, test_set, configs)\n",
        "    trainer.train()\n",
        "\n",
        "# def get_model(configs):\n",
        "#     try:\n",
        "#         return models.__dict__[configs[\"arch\"]]\n",
        "#     except KeyError:\n",
        "#         return vgg19_.__dict__[configs[\"arch\"]]\n",
        "def get_model(configs):\n",
        "    # Assuming 'arch' in configs matches 'vgg19_bn_mask_pretrain'\n",
        "    if configs[\"arch\"] == \"vgg19_bn\":\n",
        "        # Directly return the imported model architecture\n",
        "        model = vgg19_bn(\n",
        "            in_channels=configs[\"in_channels\"],\n",
        "            num_classes=configs[\"num_classes\"],\n",
        "            progress =True\n",
        "        )\n",
        "        return model\n",
        "    else:\n",
        "        # Handle case where 'arch' does not match\n",
        "        raise ValueError(f\"Model architecture {configs['arch']} is not supported.\")\n",
        "\n",
        "def get_dataset(configs):\n",
        "    train_set = fer2013(\"train\", configs)\n",
        "    val_set = fer2013(\"val\", configs)\n",
        "    test_set = fer2013(\"test\", configs, tta=True, tta_size=10)\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   main(r\"/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/config.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpkjLd2E3xAf",
        "outputId": "536bd251-430b-41b3-d858-7e55c3b2c323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU(inplace=True)\n",
            "  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu3): ReLU(inplace=True)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu4): ReLU(inplace=True)\n",
            "  (maxp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu5): ReLU(inplace=True)\n",
            "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu6): ReLU(inplace=True)\n",
            "  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu7): ReLU(inplace=True)\n",
            "  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu8): ReLU(inplace=True)\n",
            "  (maxp3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv9): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu9): ReLU(inplace=True)\n",
            "  (conv10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu10): ReLU(inplace=True)\n",
            "  (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu11): ReLU(inplace=True)\n",
            "  (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu12): ReLU(inplace=True)\n",
            "  (maxp4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu13): ReLU(inplace=True)\n",
            "  (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu14): ReLU(inplace=True)\n",
            "  (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu15): ReLU(inplace=True)\n",
            "  (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu16): ReLU(inplace=True)\n",
            "  (maxp5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            "  )\n",
            ")\n",
            "Start trainer\n",
            "{'data_path': '/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/fer_data', 'image_size': 224, 'in_channels': 3, 'num_classes': 7, 'arch': 'vgg19_bn', 'lr': 0.0001, 'weighted_loss': 0, 'momentum': 0.9, 'weight_decay': 0.001, 'distributed': 1, 'batch_size': 64, 'num_workers': 0, 'device': 'cuda:0', 'max_epoch_num': 25, 'max_plateau_count': 8, 'plateau_patience': 3, 'steplr': 50, 'log_dir': '/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/log', 'checkpoint_dir': '/content/drive/MyDrive/FeelBeat/VGG19_Segmentation/checkpoints', 'model_name': 'check', 'cwd': '/content'}\n",
            "Using device: cuda\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E001  1.333/1.136/1.136 train_acc96.986/val_acc57.780/Best val: 57.780 | p00  Time 0:03:06\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E002  1.009/1.016/1.016 train_acc124.422/val_acc62.681/Best val: 62.681 | p00  Time 0:06:12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E003  0.910/0.951/0.951 train_acc132.790/val_acc65.247/Best val: 65.247 | p00  Time 0:09:18\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E004  0.844/0.977/0.951 train_acc137.655/val_acc64.622/Best val: 65.247 | p01  Time 0:12:22\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E005  0.778/0.942/0.951 train_acc143.235/val_acc64.901/Best val: 65.247 | p02  Time 0:15:26\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E006  0.721/0.964/0.964 train_acc147.456/val_acc65.609/Best val: 65.609 | p00  Time 0:18:31\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E007  0.675/0.991/0.991 train_acc150.703/val_acc66.513/Best val: 66.513 | p00  Time 0:21:36\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E008  0.626/0.954/0.991 train_acc154.847/val_acc66.486/Best val: 66.513 | p01  Time 0:24:41\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E009  0.569/1.041/0.991 train_acc159.093/val_acc65.247/Best val: 66.513 | p02  Time 0:27:44\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E010  0.531/1.070/1.070 train_acc161.330/val_acc67.001/Best val: 67.001 | p00  Time 0:30:53\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E011  0.475/1.042/1.042 train_acc165.901/val_acc68.586/Best val: 68.586 | p00  Time 0:33:59\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E012  0.442/1.140/1.042 train_acc168.411/val_acc67.610/Best val: 68.586 | p01  Time 0:37:02\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E013  0.398/1.133/1.042 train_acc171.449/val_acc67.802/Best val: 68.586 | p02  Time 0:40:04\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E014  0.370/1.162/1.042 train_acc173.552/val_acc68.037/Best val: 68.586 | p03  Time 0:43:07\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E015  0.336/1.174/1.042 train_acc175.579/val_acc67.198/Best val: 68.586 | p04  Time 0:46:10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E016  0.208/1.377/1.377 train_acc185.192/val_acc69.200/Best val: 69.200 | p00  Time 0:49:15\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E017  0.155/1.397/1.397 train_acc189.480/val_acc69.638/Best val: 69.638 | p00  Time 0:52:20\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E018  0.127/1.514/1.397 train_acc191.382/val_acc69.529/Best val: 69.638 | p01  Time 0:55:27\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E019  0.110/1.594/1.594 train_acc192.414/val_acc69.775/Best val: 69.775 | p00  Time 0:58:32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E020  0.100/1.685/1.685 train_acc193.218/val_acc69.912/Best val: 69.912 | p00  Time 1:01:41\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E021  0.087/1.735/1.685 train_acc194.070/val_acc69.529/Best val: 69.912 | p01  Time 1:04:45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E022  0.077/1.873/1.685 train_acc194.878/val_acc69.419/Best val: 69.912 | p02  Time 1:07:48\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E023  0.069/1.903/1.685 train_acc195.415/val_acc69.556/Best val: 69.912 | p03  Time 1:10:51\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E024  0.064/1.922/1.685 train_acc195.647/val_acc69.693/Best val: 69.912 | p04  Time 1:13:54\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E025  0.054/1.961/1.685 train_acc196.562/val_acc69.666/Best val: 69.912 | p05  Time 1:16:57\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "E026  0.052/1.999/1.685 train_acc196.642/val_acc69.748/Best val: 69.912 | p06  Time 1:20:02\n",
            "\n",
            "Calc acc on private test with tta..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Us2pmtex-G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Awavo6x3tZIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}